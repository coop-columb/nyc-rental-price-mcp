"""Model implementations for NYC rental price prediction.

This module provides classes for different model types for rental price prediction.
"""

import logging
import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

logger = logging.getLogger(__name__)


class BaseModel(ABC, BaseEstimator, RegressorMixin):
    """Base class for all rental price prediction models."""

    def __init__(
        self,
        model_dir: str = "models",
        model_name: str = "base_model",
        random_state: int = 42,
    ):
        """Initialize the base model.

        Args:
            model_dir: Directory to save/load model files
            model_name: Name of the model
            random_state: Random seed for reproducibility
        """
        self.model_dir = Path(model_dir)
        self.model_name = model_name
        self.random_state = random_state
        self.model_dir.mkdir(parents=True, exist_ok=True)

        # Initialize metrics tracking
        self.metrics = {}

        logger.info(f"Initialized {self.__class__.__name__} as {model_name}")

    @abstractmethod
    def fit(self, X: pd.DataFrame, y: pd.Series) -> "BaseModel":
        """Fit the model on training data.

        Args:
            X: Training features
            y: Training target

        Returns:
            Self for method chaining
        """
        pass

    @abstractmethod
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data.

        Args:
            X: Features to predict on

        Returns:
            Array of predictions
        """
        pass

    @abstractmethod
    def save_model(self, filepath: Optional[str] = None) -> str:
        """Save the model to disk.

        Args:
            filepath: Optional path to save the model to

        Returns:
            Path where the model was saved
        """
        pass

    @abstractmethod
    def load_model(self, filepath: str) -> bool:
        """Load the model from disk.

        Args:
            filepath: Path to the saved model

        Returns:
            True if the model was loaded successfully, False otherwise
        """
        pass

    def evaluate(
        self, X: pd.DataFrame, y: pd.Series
    ) -> Dict[str, float]:
        """Evaluate the model on test data.

        Args:
            X: Test features
            y: Test target

        Returns:
            Dictionary of evaluation metrics
        """
        # Make predictions
        y_pred = self.predict(X)

        # Calculate metrics
        mae = mean_absolute_error(y, y_pred)
        mse = mean_squared_error(y, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y, y_pred)

        # Calculate percentage errors
        mape = np.mean(np.abs((y - y_pred) / y)) * 100

        # Store metrics
        self.metrics = {
            "mae": mae,
            "mse": mse,
            "rmse": rmse,
            "r2": r2,
            "mape": mape,
        }

        logger.info(
            f"Evaluation results for {self.model_name}: "
            f"MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.4f}, MAPE={mape:.2f}%"
        )

        return self.metrics

    def get_default_filepath(self) -> str:
        """Get the default filepath for saving/loading the model.

        Returns:
            Default filepath
        """
        return str(self.model_dir / f"{self.model_name}.pkl")


class GradientBoostingModel(BaseModel):
    """Gradient boosting model for rental price prediction."""

    def __init__(
        self,
        model_dir: str = "models",
        model_name: str = "gradient_boosting",
        n_estimators: int = 500,
        learning_rate: float = 0.05,
        max_depth: int = 5,
        min_samples_split: int = 5,
        min_samples_leaf: int = 2,
        subsample: float = 0.8,
        random_state: int = 42,
    ):
        """Initialize the gradient boosting model.

        Args:
            model_dir: Directory to save/load model files
            model_name: Name of the model
            n_estimators: Number of boosting stages
            learning_rate: Learning rate
            max_depth: Maximum depth of the individual regression estimators
            min_samples_split: Minimum samples required to split an internal node
            min_samples_leaf: Minimum samples required to be at a leaf node
            subsample: Fraction of samples to be used for fitting the individual base learners
            random_state: Random seed for reproducibility
        """
        super().__init__(model_dir, model_name, random_state)

        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.subsample = subsample

        # Initialize the model
        self.model = GradientBoostingRegressor(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            subsample=subsample,
            random_state=random_state,
        )

        # Feature importances
        self.feature_importances_ = None

        logger.info(
            f"Initialized GradientBoostingModel with {n_estimators} estimators, "
            f"learning_rate={learning_rate}, max_depth={max_depth}"
        )

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "GradientBoostingModel":
        """Fit the model on training data.

        Args:
            X: Training features
            y: Training target

        Returns:
            Self for method chaining
        """
        logger.info(f"Fitting {self.model_name} on {len(X)} samples")

        # Fit the model
        self.model.fit(X, y)

        # Store feature importances
        self.feature_importances_ = pd.Series(
            self.model.feature_importances_, index=X.columns
        ).sort_values(ascending=False)

        logger.info(f"Fitted {self.model_name} successfully")

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data.

        Args:
            X: Features to predict on

        Returns:
            Array of predictions
        """
        return self.model.predict(X)

    def save_model(self, filepath: Optional[str] = None) -> str:
        """Save the model to disk.

        Args:
            filepath: Optional path to save the model to

        Returns:
            Path where the model was saved
        """
        import joblib

        # Use default filepath if not provided
        if filepath is None:
            filepath = self.get_default_filepath()

        # Save the model
        joblib.dump(self.model, filepath)
        logger.info(f"Saved {self.model_name} to {filepath}")

        return filepath

    def load_model(self, filepath: Optional[str] = None) -> bool:
        """Load the model from disk.

        Args:
            filepath: Path to the saved model

        Returns:
            True if the model was loaded successfully, False otherwise
        """
        import joblib

        # Use default filepath if not provided
        if filepath is None:
            filepath = self.get_default_filepath()

        if not os.path.exists(filepath):
            logger.warning(f"Model file not found: {filepath}")
            return False

        try:
            # Load the model
            self.model = joblib.load(filepath)
            logger.info(f"Loaded {self.model_name} from {filepath}")
            return True
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            return False


class NeuralNetworkModel(BaseModel):
    """Neural network model for rental price prediction."""

    def __init__(
        self,
        model_dir: str = "models",
        model_name: str = "neural_network",
        hidden_layers: List[int] = [128, 64, 32],
        activation: str = "relu",
        dropout_rate: float = 0.3,
        learning_rate: float = 0.001,
        batch_size: int = 32,
        epochs: int = 100,
        early_stopping_patience: int = 15,
        random_state: int = 42,
    ):
        """Initialize the neural network model.

        Args:
            model_dir: Directory to save/load model files
            model_name: Name of the model
            hidden_layers: List of neurons in each hidden layer
            activation: Activation function for hidden layers
            dropout_rate: Dropout rate for regularization
            learning_rate: Learning rate for the optimizer
            batch_size: Batch size for training
            epochs: Maximum number of epochs for training
            early_stopping_patience: Patience for early stopping
            random_state: Random seed for reproducibility
        """
        super().__init__(model_dir, model_name, random_state)

        self.hidden_layers = hidden_layers
        self.activation = activation
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.early_stopping_patience = early_stopping_patience

        # Set random seeds for reproducibility
        np.random.seed(random_state)
        tf.random.set_seed(random_state)

        # Initialize the model
        self.model = None
        self.history = None
        self.feature_names = None

        logger.info(
            f"Initialized NeuralNetworkModel with hidden layers {hidden_layers}, "
            f"dropout_rate={dropout_rate}, learning_rate={learning_rate}"
        )

    def _build_model(self, input_shape: int) -> tf.keras.Model:
        """Build the neural network architecture.

        Args:
            input_shape: Number of input features

        Returns:
            Compiled Keras model
        """
        model = tf.keras.Sequential()

        # Input layer
        model.add(tf.keras.layers.Input(shape=(input_shape,)))

        # Hidden layers
        for units in self.hidden_layers:
            model.add(tf.keras.layers.Dense(units, activation=self.activation))
            model.add(tf.keras.layers.Dropout(self.dropout_rate))

        # Output layer
        model.add(tf.keras.layers.Dense(1))

        # Compile the model
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss="mse",
            metrics=["mae"],
        )

        return model

    def fit(
        self, X: pd.DataFrame, y: pd.Series, validation_data: Optional[Tuple] = None
    ) -> "NeuralNetworkModel":
        """Fit the model on training data.

        Args:
            X: Training features
            y: Training target
            validation_data: Optional tuple of (X_val, y_val)

        Returns:
            Self for method chaining
        """
        logger.info(f"Fitting {self.model_name} on {len(X)} samples")

        # Store feature names
        self.feature_names = X.columns.tolist()

        # Build the model
        self.model = self._build_model(X.shape[1])

        # Define callbacks
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor="val_loss",
                patience=self.early_stopping_patience,
                restore_best_weights=True,
            ),
            tf.keras.callbacks.ModelCheckpoint(
                str(self.model_dir / f"{self.model_name}_best.h5"),
                save_best_only=True,
            ),
        ]

        # Prepare validation data
        if validation_data is not None:
            X_val, y_val = validation_data
            validation_data = (X_val, y_val)
            validation_split = None
        else:
            validation_split = 0.2

        # Fit the model
        self.history = self.model.fit(
            X,
            y,
            validation_data=validation_data,
            validation_split=validation_split,
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1,
        )

        logger.info(f"Fitted {self.model_name} successfully")

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions on new data.

        Args:
            X: Features to predict on

        Returns:
            Array of predictions
        """
        if self.model is None:
            raise ValueError("Model is not fitted yet")

        return self.model.predict(X).flatten()

    def save_model(self, filepath: Optional[str] = None) -> str:
        """Save the model to disk.

        Args:
            filepath: Optional path to save the model to

        Returns:
            Path where the model was saved
        """
        # Use default filepath if not provided
        if filepath is None:
            filepath = str(self.model_dir / f"{self.model_name}.h5")

        if self.model is None:
            raise ValueError("Model is not fitted yet")

        # Save the model
        self.model.save(filepath)
        logger.info(f"Saved {self.model_name} to {filepath}")

        # Save feature names
        if self.feature_names:
            feature_file = str(self.model_dir / f"{self.model_name}_features.txt")
            with open(feature_file, "w") as f:
                f.write("\n".join(self.feature_names))

        return filepath

    def load_model(self, filepath: Optional[str] = None) -> bool:
        """Load the model from disk.

        Args:
            filepath: Path to the saved model

        Returns:
            True if the model was loaded successfully, False otherwise
        """
        # Use default filepath if not provided
        if filepath is None:
            filepath = str(self.model_dir / f"{self.model_name}.h5")

        if not os.path.exists(filepath):
            logger.warning(f"Model file not found: {filepath}")
            return False

        try:
            # Load the model
            self.model = tf.keras.models.load_model(filepath)
            logger.info(f"Loaded {self.model_name} from {filepath}")

            # Load feature names
            feature_file = str(self.model_dir / f"{self.model_name}_features.txt")
            if os.path.exists(feature_file):
                with open(feature_file, "r") as f:
                    self.feature_names = [line.strip() for line in f]

            return True
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            return False


class ModelEnsemble(BaseModel):
    """Ensemble of multiple models for rental price prediction."""

    def __init__(
        self,
        models: Optional[List[BaseModel]] = None,
        weights: Optional[List[float]] = None,
        model_dir: str = "models",
        model_name: str = "ensemble",
        random_state: int = 42,
    ):
        """Initialize the model ensemble.

        Args:
            models: List of models to ensemble
            weights: List of weights for each model
            model_dir: Directory to save/load model files
            model_name: Name of the model
            random_state: Random seed for reproducibility
        """
        super().__init__(model_dir, model_name, random_state)

        self.models = models or []
        self.weights = weights

        # If weights are not provided, use equal weights
        if self.weights is None and self.models:
            self.weights = [1.0 / len(self.models)] * len(self.models)

        logger.info(
            f"Initialized ModelEnsemble with {len(self.models)} models"
        )

    def add_model(self, model: BaseModel, weight: float = 1.0) -> "ModelEnsemble":
        """Add a model to the ensemble.

        Args:
            model: Model to add
            weight: Weight for the model

        Returns:
            Self for method chaining
        """
        self.models.append(model)

        # Update weights
        if self.weights is None:
            self.weights = [1.0] * len(self.models)
        else:
            self.weights.append(weight)

            # Normalize weights
            total = sum(self.weights)
            self.weights = [w / total for w in self.weights]

        logger.info(
            f"Added {model.model_name} to ensemble, now has {len(self.models)} models"
        )

        return self

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "ModelEnsemble":
        """Fit all models in the ensemble on training data.

        Args:
            X: Training features
            y: Training target

        Returns:
            Self for method chaining
        """
        logger.info(f"Fitting {len(self.models)} models in ensemble")

        for i, model in enumerate(self.models):
            logger.info(f"Fitting model {i+1}/{len(self.models)}: {model.model_name}")
            model.fit(X, y)

        logger.info(f"Fitted all models in ensemble")

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Make predictions by combining all models in the ensemble.

        Args:
            X: Features to predict on

        Returns:
            Array of predictions
        """
        if not self.models:
            raise ValueError("No models in ensemble")

        # Get predictions from each model
        predictions = []
        for model in self.models:
            pred = model.predict(X)
            predictions.append(pred)

        # Combine predictions using weights
        ensemble_pred = np.zeros(len(X))
        for i, pred in enumerate(predictions):
            ensemble_pred += pred * self.weights[i]

        return ensemble_pred

    def save_model(self, filepath: Optional[str] = None) -> str:
        """Save the ensemble to disk.

        Args:
            filepath: Optional path to save the ensemble to

        Returns:
            Path where the ensemble was saved
        """
        import joblib

        # Use default filepath if not provided
        if filepath is None:
            filepath = self.get_default_filepath()

        # Create directory for ensemble
        ensemble_dir = Path(filepath).with_suffix("")
        ensemble_dir.mkdir(parents=True, exist_ok=True)

        # Save ensemble metadata
        metadata = {
            "model_names": [model.model_name for model in self.models],
            "weights": self.weights,
        }

        metadata_file = ensemble_dir / "metadata.pkl"
        joblib.dump(metadata, metadata_file)

        # Save each model
        for i, model in enumerate(self.models):
            model_file = ensemble_dir / f"{model.model_name}.pkl"
            model.save_model(str(model_file))

        logger.info(f"Saved ensemble with {len(self.models)} models to {ensemble_dir}")

        return str(ensemble_dir)

    def load_model(self, filepath: Optional[str] = None) -> bool:
        """Load the ensemble from disk.

        Args:
            filepath: Path to the saved ensemble

        Returns:
            True if the ensemble was loaded successfully, False otherwise
        """
        import joblib

        # Use default filepath if not provided
        if filepath is None:
            filepath = self.get_default_filepath()

        # Convert to Path object
        filepath = Path(filepath)

        # If filepath is a file, assume it's the metadata file
        if filepath.is_file():
            ensemble_dir = filepath.parent
        else:
            # If filepath is a directory, look for metadata file
            ensemble_dir = filepath
            filepath = ensemble_dir / "metadata.pkl"

        if not filepath.exists():
            logger.warning(f"Ensemble metadata file not found: {filepath}")
            return False

        try:
            # Load metadata
            metadata = joblib.load(filepath)

            model_names = metadata["model_names"]
            self.weights = metadata["weights"]

            # Load each model
            self.models = []
            for name in model_names:
                # Determine model type from name
                if "neural_network" in name:
                    model = NeuralNetworkModel(model_name=name)
                    model_file = ensemble_dir / f"{name}.h5"
                else:
                    model = GradientBoostingModel(model_name=name)
                    model_file = ensemble_dir / f"{name}.pkl"

                # Load the model
                if model.load_model(str(model_file)):
                    self.models.append(model)
                else:
                    logger.warning(f"Failed to load model: {name}")

            logger.info(f"Loaded ensemble with {len(self.models)} models")

            return len(self.models) > 0
        except Exception as e:
            logger.error(f"Error loading ensemble: {str(e)}")
            return False
